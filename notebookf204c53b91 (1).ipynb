{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\n\n# Загружаем модель и процессор CLIP\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# Загружаем изображение\nimage = Image.open(\"path/to/your/image.jpg\")\n\n# Ввод текста\ntext = [\"Описание 1\", \"Описание 2\", \"Описание 3\"]\n\n# Преобразуем данные\ninputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n\n# Получаем предсказания\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image  # Результат для изображений\n    probs = logits_per_image.softmax(dim=1)  # Вероятности для каждого текста\n\nprint(probs)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install transformers torch pandas pillow","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\n\n# Загружаем модель и процессор CLIP\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# Загружаем датасет\n# Ожидается, что в CSV есть колонки \"image_path\" и \"text\" (путь к изображению и текстовое описание)\ndataset = pd.read_csv(\"path/to/your/dataset.csv\")\n\n# Список для хранения результатов\nresults = []\n\n# Обработка датасета\nfor idx, row in tqdm(dataset.iterrows(), total=len(dataset)):\n    # Загружаем изображение\n    image_path = row['image_path']\n    text = [row['text']]\n    \n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n    except Exception as e:\n        print(f\"Ошибка при загрузке изображения {image_path}: {e}\")\n        continue\n\n    # Преобразуем данные для модели\n    inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n\n    # Получаем предсказания\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits_per_image = outputs.logits_per_image  # Результаты для изображения\n        probs = logits_per_image.softmax(dim=1)  # Вероятности соответствия тексту\n\n    # Сохраняем результаты для текущей строки\n    results.append({\n        \"image_path\": image_path,\n        \"text\": text[0],\n        \"probability\": probs[0].item()\n    })\n\n# Конвертируем результаты в DataFrame и сохраняем в CSV\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"clip_inference_results.csv\", index=False)\n\nprint(\"Инференс завершен. Результаты сохранены в clip_inference_results.csv\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\n\n# Загружаем предварительно обученную модель и процессор\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# Переключаем модель в режим обучения\nmodel.train()\n\n# Проверка на наличие GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomClipDataset(Dataset):\n    def __init__(self, csv_file):\n        self.data = pd.read_csv(csv_file)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image_path = self.data.iloc[idx]['image_path']\n        text = self.data.iloc[idx]['text']\n        \n        # Загружаем изображение\n        image = Image.open(image_path).convert(\"RGB\")\n        \n        return image, text\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Параметры обучения\nbatch_size = 8\nnum_epochs = 3\nlearning_rate = 1e-5\n\n# Загрузка данных\ndataset = CustomClipDataset(\"path/to/your/dataset.csv\")\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Оптимизатор\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# Цикл обучения\nfor epoch in range(num_epochs):\n    total_loss = 0\n    for images, texts in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        \n        # Преобразуем данные и отправляем на устройство\n        inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True).to(device)\n        \n        # Прямой проход\n        outputs = model(**inputs)\n        logits_per_image = outputs.logits_per_image\n        logits_per_text = outputs.logits_per_text\n        \n        # Целевые метки для обучения (индексная матрица, где каждый элемент соответствует своему изображению/тексту)\n        labels = torch.arange(len(images)).to(device)\n\n        # Вычисляем потери\n        loss_img = torch.nn.functional.cross_entropy(logits_per_image, labels)\n        loss_txt = torch.nn.functional.cross_entropy(logits_per_text, labels)\n        loss = (loss_img + loss_txt) / 2\n\n        # Обратное распространение ошибки\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(dataloader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}\")\n\n# Сохранение модели\nmodel.save_pretrained(\"clip_finetuned_model\")\nprint(\"Модель дообучена и сохранена в clip_finetuned_model\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer\n\nclass PairDataset(Dataset):\n    def __init__(self, file_path, tokenizer, max_length=128):\n        self.data = pd.read_csv(file_path)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text_a = self.data.iloc[idx][\"text_a\"]\n        text_b = self.data.iloc[idx][\"text_b\"]\n        label = self.data.iloc[idx][\"label\"]\n\n        # Токенизируем оба текста\n        encoding_a = self.tokenizer(\n            text_a,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        encoding_b = self.tokenizer(\n            text_b,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids_a\": encoding_a[\"input_ids\"].squeeze(),\n            \"attention_mask_a\": encoding_a[\"attention_mask\"].squeeze(),\n            \"input_ids_b\": encoding_b[\"input_ids\"].squeeze(),\n            \"attention_mask_b\": encoding_b[\"attention_mask\"].squeeze(),\n            \"label\": torch.tensor(label, dtype=torch.float)\n        }\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModel\n\n# Загружаем модель и токенизатор\ntokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large-instruct\")\nmodel = AutoModel.from_pretrained(\"intfloat/multilingual-e5-large-instruct\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef contrastive_loss(embeddings_a, embeddings_b, labels, margin=1.0):\n    cos_sim = F.cosine_similarity(embeddings_a, embeddings_b)\n    # Контрастивная потеря с целевой меткой\n    loss = torch.mean((1 - labels) * F.relu(margin - cos_sim) + labels * (1 - cos_sim))\n    return loss\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import PreTrainedModel\nfrom torch import nn\n\nclass ContrastiveModel(PreTrainedModel):\n    def __init__(self, model):\n        super().__init__(model.config)\n        self.model = model\n\n    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b, labels=None):\n        # Получаем эмбеддинги для текстов A и B\n        embeddings_a = self.model(input_ids=input_ids_a, attention_mask=attention_mask_a).last_hidden_state[:, 0, :]\n        embeddings_b = self.model(input_ids=input_ids_b, attention_mask=attention_mask_b).last_hidden_state[:, 0, :]\n        \n        # Если есть метки, вычисляем потерю\n        if labels is not None:\n            loss = contrastive_loss(embeddings_a, embeddings_b, labels)\n            return {\"loss\": loss, \"embeddings_a\": embeddings_a, \"embeddings_b\": embeddings_b}\n        \n        return {\"embeddings_a\": embeddings_a, \"embeddings_b\": embeddings_b}\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\n# Создаем кастомную модель\ncontrastive_model = ContrastiveModel(model)\n\n# Параметры обучения\ntraining_args = TrainingArguments(\n    output_dir=\"./contrastive_model\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01\n)\n\n# Загрузка датасета\ntrain_dataset = PairDataset(\"path/to/your/dataset.csv\", tokenizer)\n\n# Инициализация Trainer\ntrainer = Trainer(\n    model=contrastive_model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\n# Запуск обучения\ntrainer.train()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\n# Создаем кастомную модель\ncontrastive_model = ContrastiveModel(model)\n\n# Параметры обучения\ntraining_args = TrainingArguments(\n    output_dir=\"./contrastive_model\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01\n)\n\n# Загрузка датасета\ntrain_dataset = PairDataset(\"path/to/your/dataset.csv\", tokenizer)\n\n# Инициализация Trainer\ntrainer = Trainer(\n    model=contrastive_model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\n\n# Запуск обучения\ntrainer.train()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install transformers torch sentence-transformers pandas","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer\nimport torch\n\nclass TextSimilarityDataset(Dataset):\n    def __init__(self, file_path, tokenizer, max_length=128):\n        self.data = pd.read_csv(file_path)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text_a = self.data.iloc[idx][\"text_a\"]\n        text_b = self.data.iloc[idx][\"text_b\"]\n        label = self.data.iloc[idx][\"label\"]\n\n        # Токенизируем тексты\n        encoding = self.tokenizer(\n            text_a, text_b,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n            \"label\": torch.tensor(label, dtype=torch.float)\n        }\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModel\n\n# Загружаем модель и токенизатор\ntokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large-instruct\")\nmodel = AutoModel.from_pretrained(\"intfloat/multilingual-e5-large-instruct\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass CosineSimilarityLoss(nn.Module):\n    def __init__(self, margin=0.5):\n        super(CosineSimilarityLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, embeddings_a, embeddings_b, labels):\n        cos_sim = F.cosine_similarity(embeddings_a, embeddings_b)\n        # Вычисляем потери для похожих и непохожих пар\n        loss = torch.mean((1 - labels) * torch.relu(self.margin - cos_sim) + labels * (1 - cos_sim))\n        return loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import PreTrainedModel\n\nclass SimilarityModel(PreTrainedModel):\n    def __init__(self, model):\n        super().__init__(model.config)\n        self.model = model\n        self.loss_fn = CosineSimilarityLoss()\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        # Разделяем на две части\n        embeddings = self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n        embeddings_a, embeddings_b = embeddings[::2], embeddings[1::2]\n        \n        if labels is not None:\n            # Потеря для обучения\n            loss = self.loss_fn(embeddings_a, embeddings_b, labels[::2])\n            return {\"loss\": loss}\n        \n        return {\"embeddings_a\": embeddings_a, \"embeddings_b\": embeddings_b}\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\n# Параметры обучения\ntraining_args = TrainingArguments(\n    output_dir=\"./similarity_model\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01\n)\n\n# Загрузка датасета\ntrain_dataset = TextSimilarityDataset(\"path/to/your/dataset.csv\", tokenizer)\n\n# Инициализация Trainer\ntrainer = Trainer(\n    model=SimilarityModel(model),\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\n# Запуск обучения\ntrainer.train()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sentence_transformers import InputExample\nfrom torch.utils.data import DataLoader\n\n# Шаг 1: Загрузка данных\ndef load_data(file_path):\n    data = pd.read_csv(file_path)\n    # Проверим наличие необходимых колонок\n    assert all(col in data.columns for col in [\"text_a\", \"text_b\", \"label\"]), \"Необходимые колонки отсутствуют.\"\n    return data\n\n# Шаг 2: Очистка данных\ndef clean_data(data):\n    # Удалим строки с пропущенными значениями\n    data = data.dropna(subset=[\"text_a\", \"text_b\", \"label\"])\n    # Проверим, что метки имеют только допустимые значения (0 и 1)\n    data = data[(data['label'] == 0) | (data['label'] == 1)]\n    return data\n\n# Шаг 3: Преобразование данных в формат InputExample\ndef create_examples(data):\n    examples = [\n        InputExample(texts=[row['text_a'], row['text_b']], label=float(row['label']))\n        for _, row in data.iterrows()\n    ]\n    return examples\n\n# Шаг 4: Создание DataLoader\ndef create_dataloader(examples, batch_size=16):\n    dataloader = DataLoader(examples, shuffle=True, batch_size=batch_size)\n    return dataloader\n\n# Основной код для обработки и подготовки данных\nfile_path = \"path/to/your/dataset.csv\"\ndata = load_data(file_path)\ndata = clean_data(data)\nexamples = create_examples(data)\ntrain_dataloader = create_dataloader(examples)\n\nprint(f\"Количество примеров для обучения: {len(examples)}\")\n","metadata":{},"execution_count":null,"outputs":[]}]}